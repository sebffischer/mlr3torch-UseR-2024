---
pagetitle: "`mlr3torch`"
subtitle: "Deep Learning in R with mlr3 and torch"
author: "Sebastian Fischer, Martin Binder, Bernd Bischl, Lukas Burk and others"
bibliography: references.bib
---

<h1> `mlr3torch` </h1>

<h2> Deep Learning in R with mlr3 and torch </h2>

<hr>

<h3> Sebastian Fischer, Martin Binder, Prof. Dr. Bernd Bischl, Lukas Burk and others</h3>

<h3> 2024-07-10 </h3>
<br>

![](images/lmu-logo.png){.absolute top=425 left=1100 width="300"}
![](images/MCML_Logo.jpg){.absolute top=680 left=1250 width="300"}
![](images/mardi-logo.png){.absolute top=680 left=500 width="500"}


```{r setup, include = FALSE}
library(mlr3)
library(mlr3learners)
library(mlr3torch)
library(mlr3pipelines)
library(torch)
library(mlr3mbo)
library(mlr3tuning)
options(
    datatable.print.class = FALSE,
    datatable.print.keys = FALSE,
    datatable.print.nrows = 10,
    digits = 2,
    width = 68
)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
options(mlr3torch.cache = TRUE)

set.seed(123)

knitr::opts_chunk$set(
  comment = '', fig.width = 6, fig.height = 6,
  echo = TRUE
)
```


## Introduction

* `mlr3torch` is a high level deep learning framework in R, built mainly on top of:

  * `mlr3` - A machine learning framework in R
  * `mlr3pipelines` - A *dataflow programming* toolkit
  * `torch` - A PyTorch implementation written in native R

* It allows to easily build, train and evaluate deep learning models
* GitHub: [https://github.com/mlr-org/mlr3torch](https://github.com/mlr-org/mlr3torch)

## Deep Learning in R

* `tensorflow`, `keras` & `rtorch` - use python libraries through reticulate
* `torch` - native R library
* `luz` - higher level deep learning library built on top of `torch`
* `brulee` - high level modeling functions for 'torch'

## The `mlr3` Ecosystem

<br>
<br>

![](images/mlr3_ecosystem.png){fig-align="center"}


## `mlr3`'s "Hello World"

![](images/mlr-logo-cropped.png){.absolute top=-50 left=1300 width="100"}

:::{.columns}
::: {.column width="50%"}
* `tsk()` creates an example `Task`, where the goal is to predict the miles-per-galleon of cars
* `lrn()` defines a simple regression tree `Learner` from the {rpart} package
* `rsmp()` sets a `Resampling` strategy
* `resample()` runs the resample experiment
* `msr()` initializes an mlr3 performance `Measure`
:::
::: {.column width="50%"}

```{r}
tsk_mtcars = tsk("mtcars")
lrn_tree = lrn("regr.rpart")
rsmp_cv = rsmp("cv", folds = 3)

rr = resample(
  task       = tsk_mtcars,
  learner    = lrn_tree,
  resampling = rsmp_cv
)

rr$aggregate(msr("regr.mse"))
```
:::
:::


## Dataflow Programming with `mlr3pipelines`

![](images/pipelines_logo.png){.absolute top=-50 left=1300 width="100"}

:::{.columns}
:::{.column width="50%"}
![Sequential Preprocessing](images/simple_preprocessing.png){fig-align="center"}

:::
:::{.column width="50%"}
* `mlr3pipelines`^[M. Binder and B. Bischl et al, JMLR, 2021] allows to assemble new `Learner`s by connecting `PipeOp`s in a `Graph`, e.g. for preprocessing
* `PipeOp`s are created via `po()`, and combined using the chain operator `%>>%`
<br>

```{r, output = FALSE}
library(mlr3pipelines)
library(mlr3learners)

graph = po("encode", method = "one-hot") %>>%
  po("pca") %>>%
  lrn("classif.log_reg")

learner = as_learner(graph)
learner
```
:::
:::


## `torch`'s "Hello World"

![](images/torch.png){.absolute top=-50 left=1300 width="100"}

:::{.columns}
:::{.column width="60%"}
* Support for GPU accelerated tensor operations
* An autograd system
* Provides many tensor operations and optimizers
* Extension for images via `torchvision`
* Developed by Daniel Falbel (Posit)
* GitHub: [https://github.com/mlverse/torch](https://github.com/mlverse/torch)

:::

:::{.column width="40%"}
```{r}
library(torch)
x = torch_tensor(1, requires_grad = TRUE)
w = torch_tensor(2, requires_grad = TRUE)
b = torch_tensor(3, requires_grad = TRUE)
y = w * x + b
y$backward()
x$grad
w$grad
b$grad
```
:::
:::

## `mlr3torch` in a Nutshell


![](images/mlr3torch-logo.svg){.absolute top=-50 left=1300 width="100"}

- Task Types:
    - Classification 
    - Regression
    
- Learners:
    - Off-the-shelf architectures as predefined `Learner`s
    - Build architectures as `mlr3pipelines::Graph`s
    - Customization of training via a callback mechanism
    
- Data Types:
    - Tabular data
    - Generic `torch_tensor`s
    - Multi-modal data

## Predefined architectures

:::{.columns}

::: {.column width="50%"}
* First, we construct and resample a multi layer perceptron with one hidden layer of size 10 and `ReLU` activation
* Then, we define a simple benchmark experiment that compares the neural network with the decision tree from earlier
* For this task, the neural network seems to be the wrong choice!
:::

::: {.column width="50%"}
```{r}
library(mlr3torch)
lrn_mlp = lrn("regr.mlp",
  activation     = torch::nn_relu,
  neurons        = 10,
  batch_size     = 32,
  epochs         = 100,
  loss           = t_loss("mse"),
  optimizer      = t_opt("adam", lr = 0.5),
  callbacks      = t_clbk("history")
)

design = benchmark_grid(
  tsk_mtcars, list(lrn_mlp, lrn_tree), rsmp_cv
)
bmr = benchmark(design)
bmr$aggregate(msr("regr.mse"))
```
:::
:::


## Neural Networks as `mlr3pipelines::Graph`s

:::{.columns}
:::{.column width="50%"}
![](images/simple-architecture.png){fig-align="center"}
:::
:::{.column width="50%"}
<br>
<br>
<br>

* We can build the same architecture as before by connecting `PipeOp`s in a `Graph`
* This `Graph` is fully interoperable with other `PipeOp`s

```{r, eval = TRUE, echo = TRUE, output = FALSE}
mlp_graph = po("torch_ingress_ltnsr") %>>%
  po("nn_linear", out_features = 20) %>>%
  po("nn_relu") %>>%
  po("nn_head") %>>%
  po("torch_loss", t_loss("cross_entropy")) %>>%
  po("torch_optimizer", t_opt("adam", lr = 0.1)) %>>%
  po("torch_model_classif", batch_size = 16, epochs = 5)

lrn_mlp_graph = as_learner(mlp_graph)
```
:::
:::

```{r, include = FALSE}
lrn_mlp_graph$param_set$set_values(
  torch_model_classif.epochs = 0
)
lrn_mlp_graph$predict_sets = character()
```

## Non-tabular data as `lazy_tensor`s

:::{.columns}
:::{.column width="50%"}
* The images of the MNIST task are represented as `lazy_tensor`s, which wrap a `torch::dataset`
* We have to reshape the MNIST images to work with our MLP that expects 2d inputs
* The preprocessing of `lazy_tensor`s happens lazily by internally building up a preprocessing `Graph`
* We can combine the flattening step with our previous graph into a new `GraphLearner`
:::
:::{.column width="50%"}
```{r}
tsk_mnist = tsk("mnist")
tsk_mnist$head(3)

flattener = po("trafo_reshape", shape = c(-1, 28 * 28))
flattener$train(list(tsk_mnist))[[1L]]$head(3)

lrn_flat_mlp = as_learner(flattener %>>% mlp_graph)
lrn_flat_mlp$train(tsk_mnist)
```
<br>
:::
:::

## Hyperparameter Tuning

:::{.columns}
:::{.column width="40%"}

* The resulting `GraphLearner` has a parameter set representing all configuration options, which can be tuned!
* Optimize the latent dimension of the network and the learning rate of the optimizer using Bayesian Optimization from `mlr3mbo`.

:::
:::{.column width="60%"}
```{r}
library(mlr3tuning)
library(mlr3mbo)

as.data.table(lrn_flat_mlp$param_set)[c(5, 12), 1:4]
lrn_flat_mlp$param_set$set_values(
  nn_linear.out_features = to_tune(10, 100),
  torch_optimizer.lr = to_tune(1e-05, 1e-03, logscale = TRUE)
)
```

```{r, include = FALSE}
tune(
  learner = lrn_flat_mlp,
  task = tsk_mnist$clone(deep = TRUE)$filter(1:10),
  tuner = tnr("mbo"),
  term_evals = 5L,
  resampling = rsmp("holdout")
)
```

```{r, eval = FALSE}
tune(
  learner = lrn_flat_mlp,
  task = tsk_mnist,
  tuner = tnr("mbo"),
  term_evals = 100L,
  resampling = rsmp("holdout")
)
```

:::
:::



## Multi Modal Data

```{r, include = FALSE}
set.seed(1)
dat = data.table(
  age = as.double(c(NA, sample(18:60, 99, replace = TRUE))),
  lesion_size = runif(100, 0.1, 1),
  image = as_lazy_tensor(torch_tensor(array(runif(100 * 3 * 64 * 64), dim = c(100, 3, 64, 64)))),
  status = sample(c("malignant", "benign"), 100, TRUE, c(0.1, 0.9))
)

medical_task = as_task_classif(dat, label = "Medical Diagnosis", target = "status", id = "medical")
```


:::{.columns}
::: {.column width="50%"}
* `mlr3torch` naturally supports multi-modal data as `lazy_tensor`s can be stored in `data.frame`s
* Consider a classification task with some medical data about patients, as well as images
* `mlr3torch` allows to easily build neural networks with multiple inputs, e.g. each operating on a different subset of features
:::
::: {.column width="50%"}

```{r}
medical_task
medical_task$head(3)
```
:::
:::



## Multi Modal Data

:::{.columns}
:::{.column width="40%"}
![](images/complex-graph.png){fig-align="center"}
:::

:::{.column width="60%"}
<br>
<br>

```{r, output = FALSE}
branch_num = po("select_1", selector = selector_type("numeric")) %>>%
  po("pca") %>>%
  po("torch_ingress_num") %>>%
  po("nn_linear", out_features = 20) %>>%
  po("nn_sigmoid")

branch_img = po("select_2", selector = selector_type("lazy_tensor")) %>>%
  po("torch_ingress_ltnsr") %>>%
  po("nn_conv2d", out_channels = 3) %>>%
  po("nn_relu") %>>%
  po("nn_flatten")

graph = po("imputeoor") %>>% list(branch_num, branch_img) %>>%
  po("nn_merge_cat") %>>%
  po("nn_head") %>>%
  po("torch_loss", t_loss("cross_entropy")) %>>%
  po("torch_optimizer", t_opt("adam", lr = 1e-4)) %>>%
  po("torch_model_classif", batch_size = 16, epochs = 50)
```
:::
:::


## Learn More

* This presentation: [https://github.com/sebffischer/mlr3torch-UseR-2024](https://github.com/sebffischer/mlr3torch-UseR-2024)
* The mlr3 book: [https://mlr3book.mlr-org.com/](https://mlr3book.mlr-org.com/)
* The mlr3 website: [https://mlr-org.com/](https://mlr-org.com/)
* The mlr3torch package website [https://mlr3torch.mlr-org.com/](https://mlr3torch.mlr-org.com/)
